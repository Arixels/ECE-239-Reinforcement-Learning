{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tensorforce_double_dqn.ipynb","provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMW1OmD++dfUO2VU+fK93bO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"code","metadata":{"id":"Ej-YLTku83WI","colab_type":"code","outputId":"906ac809-0e92-4f75-8811-6463a8500221","executionInfo":{"status":"ok","timestamp":1590785310141,"user_tz":420,"elapsed":4042,"user":{"displayName":"DANIEL TRUONG","photoUrl":"","userId":"04409221502624938347"}},"colab":{"base_uri":"https://localhost:8080/","height":366}},"source":["!pip install tensorforce==0.4.4"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting tensorforce==0.4.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7e/03/c1bf669cfdaf18c5ef71f4bef32252624a937d63d87ec39c951e69681bd0/tensorforce-0.4.4-py3-none-any.whl (269kB)\n","\u001b[K     |████████████████████████████████| 276kB 2.7MB/s \n","\u001b[?25hRequirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from tensorforce==0.4.4) (3.6.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorforce==0.4.4) (1.12.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorforce==0.4.4) (1.18.4)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from tensorforce==0.4.4) (1.4.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorforce==0.4.4) (4.41.1)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->tensorforce==0.4.4) (1.8.1)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->tensorforce==0.4.4) (0.7.1)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->tensorforce==0.4.4) (1.4.0)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->tensorforce==0.4.4) (8.3.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->tensorforce==0.4.4) (46.4.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->tensorforce==0.4.4) (19.3.0)\n","Installing collected packages: tensorforce\n","  Found existing installation: tensorforce 0.4.0\n","    Uninstalling tensorforce-0.4.0:\n","      Successfully uninstalled tensorforce-0.4.0\n","Successfully installed tensorforce-0.4.4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Xa2s_uB8Ikgj","colab_type":"code","outputId":"eb266ab0-bf8f-4e82-ff3f-f51c78127ae0","executionInfo":{"status":"ok","timestamp":1590721946959,"user_tz":420,"elapsed":9648,"user":{"displayName":"DANIEL TRUONG","photoUrl":"","userId":"04409221502624938347"}},"colab":{"base_uri":"https://localhost:8080/","height":788}},"source":["#connect to Google Colab TPUs\n","%tensorflow_version 2.x\n","import tensorflow as tf\n","print(\"Tensorflow version \" + tf.__version__)\n","\n","try:\n","  tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n","except ValueError:\n","  raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n","\n","tf.config.experimental_connect_to_cluster(tpu)\n","tf.tpu.experimental.initialize_tpu_system(tpu)\n","tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Tensorflow version 2.2.0\n","Running on TPU  ['10.26.91.82:8470']\n","INFO:tensorflow:Initializing the TPU system: grpc://10.26.91.82:8470\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Initializing the TPU system: grpc://10.26.91.82:8470\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Clearing out eager caches\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Clearing out eager caches\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Finished initializing TPU system.\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Finished initializing TPU system.\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:Found TPU system:\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:Found TPU system:\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores: 8\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Workers: 1\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"],"name":"stderr"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stdout"},{"output_type":"stream","text":["INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"ihSkPvUt9OXt","colab_type":"code","colab":{}},"source":["import tensorforce\n","from tensorforce.environments import Environment\n","#from tensorforce.agents import Agent\n","#from tensorforce.execution import Runner\n","#from tensorforce.core.networks import LayeredNetwork\n","#import tensorforce.environments\n","import os\n","import gym\n","from gym import wrappers\n","import numpy as np\n","import matplotlib.pyplot as plt\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GgZK2efBwL3D","colab_type":"code","colab":{}},"source":["# Modify/Set Hyperparameters for NN's \n","class hasselt_hyp:\n","  '''\n","  Hyperparameters according to DDQN paper from Hasselt et al 2015\n","  '''\n","  def __init__(self):\n","    #self.optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.00025,\n","                                                # momentum=0.95)\n","    self.learning_rate=0.00025\n","    self.momentum=0.95\n","    self.num_iterations = 20000\n","    self.target_update_period = 10000\n","    self.gamma = 0.99\n","    self.replay_buffer_max_length = 1000000\n","    self.collect_steps_per_iteration = 4 \n","    self.batch_size = 32\n","    self.log_interval = 20000\n","    self.num_eval_episodes = 50\n","    self.eval_interval = 1000000\n","    self.size_of_replay_memory=30000\n","    #calvin - I'm adding these hyperparams?\n","    self.epsilon = 0.1\n","    self.epsilon_decay_period=1000000"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VIbanC8f_ytB","colab_type":"code","outputId":"c1dd471e-45e8-429b-a19f-54d0fb7d6737","executionInfo":{"status":"error","timestamp":1590785630336,"user_tz":420,"elapsed":440,"user":{"displayName":"DANIEL TRUONG","photoUrl":"","userId":"04409221502624938347"}},"colab":{"base_uri":"https://localhost:8080/","height":393}},"source":["#apply wrappers\n","print(tensorforce.__version__)\n","hasselt = hasselt_hyp()\n","print(dir(tensorforce.core))\n","def create_env(game='Alien-v0'):\n","  print('Creating env for {}'.format(game))\n","  '''env = gym.make(game)\n","  env = wrappers.atari_preprocessing.AtariPreprocessing(env,frame_skip=1)\n","  env = wrappers.frame_stack.FrameStack(env,4)'''\n","  return env\n","\n","env = create_env()"],"execution_count":15,"outputs":[{"output_type":"stream","text":["0.4.4\n","['__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'explorations', 'memories', 'preprocessors']\n","Creating env for Alien-v0\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-dae37db911bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-15-dae37db911bc>\u001b[0m in \u001b[0;36mcreate_env\u001b[0;34m(game)\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matari_preprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAtariPreprocessing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mframe_skip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   env = wrappers.frame_stack.FrameStack(env,4)'''\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"]}]},{"cell_type":"code","metadata":{"id":"ikUZexXyWRQ5","colab_type":"code","colab":{}},"source":["#create network\n","'''The convolution network used in the experiment is exactly the one\n","proposed by proposed by Mnih et al. (2015), we only provide de-\n","tails here for completeness. The input to the network is a 84x84x4\n","tensor containing a rescaled, and gray-scale, version of the last four\n","frames. The first convolution layer convolves the input with 32 fil-\n","ters of size 8 (stride 4), the second layer has 64 layers of size 4\n","(stride 2), the final convolution layer has 64 filters of size 3 (stride\n","1). This is followed by a fully-connected hidden layer of 512 units.\n","All these layers are separated by Rectifier Linear Units (ReLu). Fi-\n","nally, a fully-connected linear layer projects to the output of the\n","network, i.e., the Q-values. The optimization employed to train the\n","network is RMSProp (with momentum parameter 0.95).'''\n","\n","def create_network():\n","  layers = [dict(type='conv2d',size=32,window=8,stride=4),#(8x8x32) with stride 4\n","            dict(type='conv2d',size=64,window=4,stride=2),#(4x4x64) with stride 2\n","            dict(type='conv2d',size=64,window=3,stride=1),#(3x3x64) with stride 1\n","            dict(type='flatten'),\n","            dict(type='dense',size=512,activation='relu'),\n","            ]\n","  return layers"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kQ4TcPDqBbv5","colab_type":"code","outputId":"f0ffdaf3-84d5-422f-c046-0aed62d9862d","executionInfo":{"status":"error","timestamp":1590783955921,"user_tz":420,"elapsed":827,"user":{"displayName":"DANIEL TRUONG","photoUrl":"","userId":"04409221502624938347"}},"colab":{"base_uri":"https://localhost:8080/","height":380}},"source":["exploration=dict(\n","    type='decaying', unit='timesteps', decay='polynomial',\n","    initial_value=1.0, decay_steps=40000, final_value=0.1\n",")\n","agent = Agent.create(agent='dqn',\n","                     environment=env,\n","                     memory=hasselt.size_of_replay_memory,\n","                     max_episode_timesteps=3000,\n","                     learning_rate=hasselt.learning_rate,\n","                     discount=hasselt.gamma,\n","                     exploration=exploration,\n","                     target_sync_frequency=10000,\n","                     double_dqn=True,\n","                     network=dict(type='layered', layers=create_network()),\n",")"],"execution_count":6,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-87b57dee4969>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m                      \u001b[0mtarget_sync_frequency\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                      \u001b[0mdouble_dqn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                      \u001b[0mnetwork\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'layered'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m )\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorforce/agents/agent.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(agent, environment, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;31m# Keyword specification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensorforce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorforce/agents/agent.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(agent, environment, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_episode_timesteps'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_episode_timesteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m             \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menvironment\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menvironment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'double_dqn'"]}]},{"cell_type":"code","metadata":{"id":"SVEXBjvSwwva","colab_type":"code","colab":{}},"source":["# Train\n","def train_and_eval_agent(agent, env, num_episodes=100,eval_every=10):\n","  agent.reset()\n","  query = ['action-distribution-probabilities']\n","  max_q_probs = []\n","  timesteps = 0\n","  for episode in range(num_episodes):\n","      states = env.reset()\n","      terminal = False\n","      highscore=0.0\n","      cumulative=0.0\n","      while not terminal:\n","          actions, queried = agent.act(states=states,query=query)\n","          max_q_probs.append(np.max(queried))\n","          states, terminal, reward = env.execute(actions=actions)\n","          agent.observe(terminal=terminal, reward=reward)\n","          timesteps+=1\n","          cumulative+=reward\n","      print('Episode {}: Cumulative Reward: {} Timesteps: {}'.format(episode,cumulative,timesteps))  \n","\n","      if episode%eval_every==0:\n","          print('Evaluating...')\n","          highscore = evaluate_agent(agent,env,highscore,num_episodes=5)\n","  return max_q_probs\n","# Evaluate\n","def evaluate_agent(agent, env, highscore, num_episodes=5):\n","  sum_rewards = 0.0\n","\n","  for _ in range(num_episodes):\n","      states = env.reset()\n","      internals = agent.initial_internals()\n","      terminal = False\n","      while not terminal:\n","          actions, internals = agent.act(states=states, internals=internals, evaluation=True)\n","          states, terminal, reward = env.execute(actions=actions)\n","          sum_rewards += reward\n","\n","  avg_score = sum_rewards / num_episodes\n","  print('Mean episode reward: {}'.format(avg_score) )\n","  if avg_score>highscore:\n","            agent.save(directory='best_agents',\n","                       filename='dqn-Alien-v0_best_eval.hdf5',\n","                       format='hdf5')\n","  return highscore\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jsphMlms0szC","colab_type":"code","outputId":"22f57f8e-4c8e-4880-8eb9-914ff6debb34","executionInfo":{"status":"ok","timestamp":1590725364433,"user_tz":420,"elapsed":2327707,"user":{"displayName":"DANIEL TRUONG","photoUrl":"","userId":"04409221502624938347"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["try:\n","  os.mkdir('best_agents')\n","except:\n","  pass\n","\n","max_q_probs = train_and_eval_agent(agent,env)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Episode 0: Cumulative Reward: 280.0 Timesteps: 1245\n","Evaluating...\n","Mean episode reward: 140.0\n","Episode 1: Cumulative Reward: 140.0 Timesteps: 2057\n","Episode 2: Cumulative Reward: 140.0 Timesteps: 3054\n","Episode 3: Cumulative Reward: 130.0 Timesteps: 3749\n","Episode 4: Cumulative Reward: 230.0 Timesteps: 4851\n","Episode 5: Cumulative Reward: 140.0 Timesteps: 5678\n","Episode 6: Cumulative Reward: 300.0 Timesteps: 6770\n","Episode 7: Cumulative Reward: 130.0 Timesteps: 7773\n","Episode 8: Cumulative Reward: 220.0 Timesteps: 8674\n","Episode 9: Cumulative Reward: 250.0 Timesteps: 9716\n","Episode 10: Cumulative Reward: 120.0 Timesteps: 10326\n","Evaluating...\n","Mean episode reward: 242.0\n","Episode 11: Cumulative Reward: 210.0 Timesteps: 11328\n","Episode 12: Cumulative Reward: 150.0 Timesteps: 12354\n","Episode 13: Cumulative Reward: 190.0 Timesteps: 13031\n","Episode 14: Cumulative Reward: 320.0 Timesteps: 13918\n","Episode 15: Cumulative Reward: 900.0 Timesteps: 15303\n","Episode 16: Cumulative Reward: 260.0 Timesteps: 16072\n","Episode 17: Cumulative Reward: 200.0 Timesteps: 17003\n","Episode 18: Cumulative Reward: 270.0 Timesteps: 17813\n","Episode 19: Cumulative Reward: 490.0 Timesteps: 18804\n","Episode 20: Cumulative Reward: 350.0 Timesteps: 19686\n","Evaluating...\n","Mean episode reward: 38.0\n","Episode 21: Cumulative Reward: 180.0 Timesteps: 20440\n","Episode 22: Cumulative Reward: 230.0 Timesteps: 21295\n","Episode 23: Cumulative Reward: 420.0 Timesteps: 22412\n","Episode 24: Cumulative Reward: 310.0 Timesteps: 23403\n","Episode 25: Cumulative Reward: 340.0 Timesteps: 24257\n","Episode 26: Cumulative Reward: 300.0 Timesteps: 24877\n","Episode 27: Cumulative Reward: 490.0 Timesteps: 25901\n","Episode 28: Cumulative Reward: 380.0 Timesteps: 26798\n","Episode 29: Cumulative Reward: 330.0 Timesteps: 27611\n","Episode 30: Cumulative Reward: 510.0 Timesteps: 28715\n","Evaluating...\n","Mean episode reward: 140.0\n","Episode 31: Cumulative Reward: 340.0 Timesteps: 29854\n","Episode 32: Cumulative Reward: 120.0 Timesteps: 30447\n","Episode 33: Cumulative Reward: 300.0 Timesteps: 31171\n","Episode 34: Cumulative Reward: 170.0 Timesteps: 31783\n","Episode 35: Cumulative Reward: 330.0 Timesteps: 32810\n","Episode 36: Cumulative Reward: 340.0 Timesteps: 33613\n","Episode 37: Cumulative Reward: 370.0 Timesteps: 34310\n","Episode 38: Cumulative Reward: 490.0 Timesteps: 35133\n","Episode 39: Cumulative Reward: 260.0 Timesteps: 35778\n","Episode 40: Cumulative Reward: 190.0 Timesteps: 36500\n","Evaluating...\n","Mean episode reward: 246.0\n","Episode 41: Cumulative Reward: 920.0 Timesteps: 37815\n","Episode 42: Cumulative Reward: 290.0 Timesteps: 38533\n","Episode 43: Cumulative Reward: 490.0 Timesteps: 39769\n","Episode 44: Cumulative Reward: 410.0 Timesteps: 40897\n","Episode 45: Cumulative Reward: 410.0 Timesteps: 41911\n","Episode 46: Cumulative Reward: 450.0 Timesteps: 42640\n","Episode 47: Cumulative Reward: 250.0 Timesteps: 43213\n","Episode 48: Cumulative Reward: 240.0 Timesteps: 43882\n","Episode 49: Cumulative Reward: 350.0 Timesteps: 44709\n","Episode 50: Cumulative Reward: 340.0 Timesteps: 45362\n","Evaluating...\n","Mean episode reward: 56.0\n","Episode 51: Cumulative Reward: 280.0 Timesteps: 46280\n","Episode 52: Cumulative Reward: 350.0 Timesteps: 47570\n","Episode 53: Cumulative Reward: 930.0 Timesteps: 48508\n","Episode 54: Cumulative Reward: 300.0 Timesteps: 49039\n","Episode 55: Cumulative Reward: 470.0 Timesteps: 49696\n","Episode 56: Cumulative Reward: 290.0 Timesteps: 50370\n","Episode 57: Cumulative Reward: 300.0 Timesteps: 51198\n","Episode 58: Cumulative Reward: 240.0 Timesteps: 51809\n","Episode 59: Cumulative Reward: 320.0 Timesteps: 52840\n","Episode 60: Cumulative Reward: 260.0 Timesteps: 53778\n","Evaluating...\n","Mean episode reward: 22.0\n","Episode 61: Cumulative Reward: 370.0 Timesteps: 54492\n","Episode 62: Cumulative Reward: 220.0 Timesteps: 55192\n","Episode 63: Cumulative Reward: 300.0 Timesteps: 55902\n","Episode 64: Cumulative Reward: 370.0 Timesteps: 56999\n","Episode 65: Cumulative Reward: 450.0 Timesteps: 57733\n","Episode 66: Cumulative Reward: 390.0 Timesteps: 58792\n","Episode 67: Cumulative Reward: 330.0 Timesteps: 59729\n","Episode 68: Cumulative Reward: 140.0 Timesteps: 60406\n","Episode 69: Cumulative Reward: 310.0 Timesteps: 61255\n","Episode 70: Cumulative Reward: 480.0 Timesteps: 62424\n","Evaluating...\n","Mean episode reward: 140.0\n","Episode 71: Cumulative Reward: 260.0 Timesteps: 62884\n","Episode 72: Cumulative Reward: 220.0 Timesteps: 63651\n","Episode 73: Cumulative Reward: 350.0 Timesteps: 64234\n","Episode 74: Cumulative Reward: 150.0 Timesteps: 64871\n","Episode 75: Cumulative Reward: 240.0 Timesteps: 65440\n","Episode 76: Cumulative Reward: 320.0 Timesteps: 66222\n","Episode 77: Cumulative Reward: 220.0 Timesteps: 66892\n","Episode 78: Cumulative Reward: 210.0 Timesteps: 67659\n","Episode 79: Cumulative Reward: 220.0 Timesteps: 68322\n","Episode 80: Cumulative Reward: 330.0 Timesteps: 69067\n","Evaluating...\n","Mean episode reward: 0.0\n","Episode 81: Cumulative Reward: 910.0 Timesteps: 70089\n","Episode 82: Cumulative Reward: 210.0 Timesteps: 70938\n","Episode 83: Cumulative Reward: 240.0 Timesteps: 71527\n","Episode 84: Cumulative Reward: 410.0 Timesteps: 72603\n","Episode 85: Cumulative Reward: 350.0 Timesteps: 73807\n","Episode 86: Cumulative Reward: 240.0 Timesteps: 74496\n","Episode 87: Cumulative Reward: 310.0 Timesteps: 75300\n","Episode 88: Cumulative Reward: 460.0 Timesteps: 76452\n","Episode 89: Cumulative Reward: 250.0 Timesteps: 77039\n","Episode 90: Cumulative Reward: 260.0 Timesteps: 77982\n","Evaluating...\n","Mean episode reward: 226.0\n","Episode 91: Cumulative Reward: 210.0 Timesteps: 78709\n","Episode 92: Cumulative Reward: 360.0 Timesteps: 79351\n","Episode 93: Cumulative Reward: 250.0 Timesteps: 80005\n","Episode 94: Cumulative Reward: 270.0 Timesteps: 80662\n","Episode 95: Cumulative Reward: 500.0 Timesteps: 81712\n","Episode 96: Cumulative Reward: 260.0 Timesteps: 82240\n","Episode 97: Cumulative Reward: 250.0 Timesteps: 82701\n","Episode 98: Cumulative Reward: 340.0 Timesteps: 83556\n","Episode 99: Cumulative Reward: 260.0 Timesteps: 84257\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3xKGumBB2fyr","colab_type":"code","colab":{}},"source":["runner = Runner(agent=agent,environment=env)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WWp26owl2o6T","colab_type":"code","outputId":"3f8a8e86-46bf-4683-8c5e-55c509dc0e7f","executionInfo":{"status":"ok","timestamp":1590713048719,"user_tz":420,"elapsed":173102,"user":{"displayName":"DANIEL TRUONG","photoUrl":"","userId":"04409221502624938347"}},"colab":{"base_uri":"https://localhost:8080/","height":53}},"source":["runner.run(num_episodes=10,evaluation=False)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Episodes: 100%|██████████| 10/10 [05:54, reward=0.00, ts/ep=666, sec/ep=2.55, ms/ts=3.8, agent=57.5%]\n","Episodes: 100%|██████████| 10/10 [02:52, reward=320.00, ts/ep=889, sec/ep=17.81, ms/ts=20.0, agent=91.1%]"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"zxxNmnajMWq7","colab_type":"code","colab":{}},"source":["# Close agent and environment\n","agent.close()\n","env.close()"],"execution_count":0,"outputs":[]}]}